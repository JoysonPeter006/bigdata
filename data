sqoop-export --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/cdb21dw011 --username cdb22dw011 -P --table karthick --export-dir /StudentData1/part*
mysql -h cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com -u cdb22dw011 -p
sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --incremental append --check-column cust_id --last-value 115 --target-dir /customer
[14:52] Karthick Selvam (Guest)
    cp /home/ubh01/apache-hive-2.3.2-bin/lib/hive-common-2.3.2.jar /home/ubh01/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
​[14:57] Karthick Selvam (Guest)
    cp /home/ubh01/apache-hive-2.3.2-bin/lib/hive-common-2.3.2.jar /home/ubh01/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
​[14:57] Karthick Selvam (Guest)
    hdfs dfs -rm -r /user/ubh01/customer
​[14:58] Karthick Selvam (Guest)
    sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
[15:22] Karthick Selvam (Guest)
    hdfs dfs -ls /customer
​[15:22] Karthick Selvam (Guest)
    hdfs dfs -get /customer/part* /home/ubh01
​[15:22] Karthick Selvam (Guest)
    cat part* > customer.txt
​[15:22] Karthick Selvam (Guest)
    hdfs dfs -mkdir /input
​[15:23] Karthick Selvam (Guest)
    hdfs dfs -put /home/ubh01/customer.txt /input
​[15:23] Karthick Selvam (Guest)
    sqoop-export --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --update-key cust_id --update-mode allowinsert --export-dir /input/customer.txt
 
 https://github.com/karthick1808/CDB21DW062/blob/main/StudentData.csv
 
 create table karthick(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
row format delimited
fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");
=============================================================================
# Load the data from HDFS Location -> move the file from original location to hive architecture local
#if you drop the data original file is lost
load data inpath '/karthick/StudentData.csv' overwrite into table karthick;
=============================================================================
#Load from the Local File System
#Keep the original file from local file system
load data local inpath '/home/ubh01/Desktop/StudentData.csv' overwrite into table karthick;
============================
create table stat_part(
age int,
gender string,
name string,
roll int,
marks string,
email string
)partitioned by (course string);
 
insert into table stat_part partition(course = 'DB') select age,gender,name,roll, marks,email from karthick where course='DB'
==========================================

Dynamic Partition

 

create table dyna_part(
age int,
gender string,
name string,
roll int,
marks string,
email string
)partitioned by (course string);

 

set hive.exec.dynamic.partition.mode=nonstrict;

 

insert into table dyna_part partition(course) select age,gender,name,roll, marks,email,course from karthick where course = 'DB';
=========================
Dynamic Partition

 

create table dyna_part(
age int,
gender string,
name string,
roll int,
marks string,
email string
)partitioned by (course string);

 

set hive.exec.dynamic.partition.mode=nonstrict;

 

insert into table dyna_part partition(course) select age,gender,name,roll, marks,email,course from karthick where course = 'DB';
=============================
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.enforce.bucketing=true;
=========================================================
create table karthick_bucket(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
clustered by(age) into 2 buckets stored as textfile;
=============================================================
create table selvam_bucket(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
clustered by(course) into 6 buckets stored as textfile;
